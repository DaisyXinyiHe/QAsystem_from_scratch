{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple transformer question and answer model needs:\n",
        "* Tokinizer (Here i am using BERT's way to tokenize beginning and end of sentences)\n",
        "* Transformer encoder\n",
        "* QA head (predict start and end position)\n",
        "* Training and inference logic\n",
        "\n"
      ],
      "metadata": {
        "id": "F64zJ7O3cbSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use self-built attention head"
      ],
      "metadata": {
        "id": "bLQxgKC9j5SV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTfvlb4ObTv1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "        \"vocab_size\": 45,\n",
        "        \"hidden_size\": 64,\n",
        "        \"max_position_embeddings\": 64,\n",
        "        \"num_attention_heads\": 4,\n",
        "        'intermediate_size':10,\n",
        "        'hidden_dropout_prob':0.01,\n",
        "        \"num_hidden_layers\": 12,\n",
        "        'mask':None\n",
        "    }"
      ],
      "metadata": {
        "id": "MBvVdTLxGQYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokinizer\n",
        "\n",
        "class SimpleTokinizer:\n",
        "  def __init__(self):\n",
        "    self.vocab = {\"[PAD]\":0, \"[CLS]\":1, \"[SEP]\":2,\"[UNK]\":3}\n",
        "    self.reverse_vocab = {0:\"[PAD]\", 1:\"[CLS]\", 2:\"[SEP]\",3:\"[UNK]\"}\n",
        "    self.idx = 4\n",
        "\n",
        "  def build_vocab(self,texts):\n",
        "    for text in texts:\n",
        "      for word in text.lower().split():\n",
        "        if word not in self.vocab:\n",
        "          self.vocab[word] = self.idx\n",
        "          self.reverse_vocab[self.idx] = word\n",
        "          self.idx+=1\n",
        "  def encode(self, question, contaxt, max_len = 64):\n",
        "    ## for each QA, input takes format of [CLS] question tokens [SEP] context tokens [SEP]\n",
        "    tokens = [\"[CLS]\"]+question.lower().split()+[\"[SEP]\"]+contaxt.lower().split()+[\"[SEP]\"]\n",
        "    token_ids = [self.vocab.get(token, self.vocab[\"[UNK]\"]) for token in tokens]\n",
        "    token_type_ids = [1]*(len(question.split())+2)+[2]*(len(context.split())+1)\n",
        "    attention_mask = [1] * len(token_ids)\n",
        "    padding = [0]*(max_len - len(token_ids))\n",
        "    # print(token_type_ids)\n",
        "    return {\n",
        "        'input_ids':torch.tensor(token_ids + padding[:max_len - len(token_ids)]),\n",
        "        'attention_mask':torch.tensor(attention_mask+padding[:max_len - len(token_ids)]),\n",
        "        'token':tokens+['[PAD]']*len(padding),\n",
        "        'token_type_ids':torch.tensor(token_type_ids+padding[:max_len - len(token_ids)]),\n",
        "\n",
        "    }"
      ],
      "metadata": {
        "id": "AWoGbLg3cehF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample\n",
        "question = \"What is KNN?\"\n",
        "context = '''KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It classifies new data points by finding the \"k\" most similar data points (neighbors) in the training data and assigning the new data point to the majority class among those neighbors.'''\n"
      ],
      "metadata": {
        "id": "sM_MmMgWGd8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenizer\n",
        "tokenizer = SimpleTokinizer()\n",
        "tokenizer.build_vocab([question, context])\n",
        "# input_ids, attention_mask = tokenizer.encode(question, context)\n",
        "inputs = tokenizer.encode(question, context)"
      ],
      "metadata": {
        "id": "ZsVyzA_WGd8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs"
      ],
      "metadata": {
        "id": "lN_yR2g3TJMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids = input_ids.unsqueeze(0)\n",
        "# attention_mask = attention_mask.unsqueeze(0)"
      ],
      "metadata": {
        "id": "hbgu9-c2J8zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config['mask'] = attention_mask"
      ],
      "metadata": {
        "id": "YYsPnOP2RGZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids.shape, attention_mask.shape"
      ],
      "metadata": {
        "id": "SUcbVzSYKz-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Atttention head\n",
        "\n",
        "def scaled_dot_product_attention(q,k,v, mask = None):\n",
        "  # print(q.shape,k.shape,v.shape)\n",
        "  dim_k = k.size(-1) ## embedding size\n",
        "  # print(dim_k)\n",
        "  # print(k.transpose(1,2).shape)\n",
        "  scores = torch.bmm(q,k.transpose(1,2)) / math.sqrt(dim_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0, -float('inf'))\n",
        "  weights = F.softmax(scores, dim=1)\n",
        "  attention_outputs = torch.bmm(weights, v)\n",
        "  return attention_outputs\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim, mask=None):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim, head_dim)\n",
        "    self.k = nn.Linear(embed_dim, head_dim)\n",
        "    self.v = nn.Linear(embed_dim, head_dim)\n",
        "    self.mask = mask\n",
        "\n",
        "  def forward(self,hidden_state):\n",
        "    attention_outputs = scaled_dot_product_attention(self.q(hidden_state),self.k(hidden_state),self.v(hidden_state), mask = self.mask)\n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    embed_dim = config['hidden_size']\n",
        "    num_heads = config['num_attention_heads']\n",
        "    head_dim = embed_dim // num_heads\n",
        "    mask = config['mask']\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embed_dim, head_dim, mask) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self,hidden_state):\n",
        "    # print(hidden_state.shape)\n",
        "    # for h in self.heads:\n",
        "    #   print(h(hidden_state)[0][0].shape)\n",
        "    # print(self.heads)\n",
        "    x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)\n",
        "    x = self.output_linear(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "upj8B52_feAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AttentionHead(config['hidden_size'], config['num_attention_heads'], config['mask'])"
      ],
      "metadata": {
        "id": "JcksK0yqYRfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attn = MultiHeadAttention(config)\n",
        "token_emb = nn.Embedding(config['vocab_size'], config['hidden_size'])\n",
        "input_embeds = token_emb(inputs['input_ids'])\n",
        "input_embeds = input_embeds.unsqueeze(0)\n",
        "attn_output = multihead_attn(input_embeds)"
      ],
      "metadata": {
        "id": "dIL-noipHAsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config['hidden_size'], config['intermediate_size'])\n",
        "    self.linear2 = nn.Linear(config['intermediate_size'], config['hidden_size'])\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "jGqt0bAug0Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_forward = FeedForward(config)"
      ],
      "metadata": {
        "id": "DKwjjqd0PTjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff_outputs = feed_forward(attn_output)"
      ],
      "metadata": {
        "id": "_S64BQZvPVG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff_outputs.shape"
      ],
      "metadata": {
        "id": "rk4RkliVPWXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.layer_norm1 = nn.LayerNorm(config['hidden_size'])\n",
        "    self.layer_norm2 = nn.LayerNorm(config['hidden_size'])\n",
        "    self.attention = MultiHeadAttention(config)\n",
        "    self.feedforward = FeedForward(config)\n",
        "  def forward(self, x):\n",
        "    hidden_state = self.layer_norm1(x)\n",
        "    atten_output =  self.attention(hidden_state)\n",
        "    x+=atten_output\n",
        "    x += self.feedforward(self.layer_norm2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "6brH7YThmSMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = TransformerEncoderLayer(config)\n",
        "print(input_embeds.shape)\n",
        "encoder_layer(input_embeds).shape"
      ],
      "metadata": {
        "id": "E_9yHmzsPZ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.token_embeddings = nn.Embedding(config['vocab_size'], config['hidden_size'])\n",
        "    self.position_embeddings = nn.Embedding(config['max_position_embeddings'], config['hidden_size'])\n",
        "    self.layer_norm = nn.LayerNorm(config['hidden_size'], eps = 1e-12)\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    seq_length = input_ids.unsqueeze(0).size(1)\n",
        "    position_ids = torch.arange(seq_length, dtype = torch.long).unsqueeze(0)\n",
        "    # print(input_ids)\n",
        "    token_embeddings = self.token_embeddings(input_ids)\n",
        "    position_embeddings = self.position_embeddings(position_ids)\n",
        "    embeddings = token_embeddings+position_embeddings\n",
        "    embeddings = self.layer_norm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "VQ4V-vC92JPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embeddings(config)\n",
        "embedding_layer(inputs['input_ids'])#.size()"
      ],
      "metadata": {
        "id": "jriCgnyGbYBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.embeddings = Embeddings(config)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [TransformerEncoderLayer(config) for _ in range(config['num_hidden_layers'])]\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.embeddings(x)\n",
        "    for layer in self.layers:\n",
        "      # print(layer)\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oMlQ6Sq14Lyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoder(config)\n",
        "encoder(inputs['input_ids']).size()"
      ],
      "metadata": {
        "id": "fkKToZBH6dRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add a QA head\n",
        "class QA_Transformer(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model = 64, max_len = 64, heads = 4):\n",
        "    super().__init__()\n",
        "    self.config = {\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"hidden_size\": d_model,\n",
        "        \"max_position_embeddings\": max_len,\n",
        "        \"num_attention_heads\": heads,\n",
        "        'intermediate_size':10,\n",
        "        'hidden_dropout_prob':0.01,\n",
        "        \"num_hidden_layers\": 12,\n",
        "        'mask':None\n",
        "    }\n",
        "    # self.embedding = nn.Embedding(self.config['vocab_size'], self.config['hidden_size'])\n",
        "    self.encoder = TransformerEncoder(self.config)\n",
        "    # self.position_embeddings = nn.Parameter(torch.randn(1, self.config['max_position_embeddings'], self.config['hidden_size']))\n",
        "    # self.position_embeddings = nn.Parameter(torch.randint(1, self.config['max_position_embeddings'], self.config['hidden_size']))\n",
        "    self.qa_outputs = nn.Linear(self.config['hidden_size'], 2)\n",
        "\n",
        "  def forward(self, input_ids):#, attention_mask):\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "    # attention_mask = attention_mask.unsqueeze(0)\n",
        "    # x = self.embedding(input_ids)+self.position_embeddings[:,:input_ids.size(1)]#.long()\n",
        "    x = self.encoder(input_ids)#, attention_mask)\n",
        "    logits = self.qa_outputs(x)\n",
        "    start_logits, end_logits = logits.split(1,dim=-1)\n",
        "    return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MpKwDhBJ4nnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample\n",
        "question = \"What is KNN?\"\n",
        "context = '''KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It classifies new data points by finding the \"k\" most similar data points (neighbors) in the training data and assigning the new data point to the majority class among those neighbors.'''\n"
      ],
      "metadata": {
        "id": "Lo3ZcEw56mP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenizer\n",
        "tokenizer = SimpleTokinizer()\n",
        "tokenizer.build_vocab([question, context])\n",
        "# input_ids, attention_mask = tokenizer.encode(question, context)\n",
        "# input_ids = input_ids.unsqueeze(0)\n",
        "# attention_mask = attention_mask.unsqueeze(0)\n",
        "input = tokenizer.encode(question, context)"
      ],
      "metadata": {
        "id": "D3ItMBvZ7RY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input['input_ids']"
      ],
      "metadata": {
        "id": "CTa0zsmO7oOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input['attention_mask'].shape, input['input_ids'].shape, len(tokenizer.vocab)"
      ],
      "metadata": {
        "id": "D_uBD0-07qi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input['input_ids'].shape"
      ],
      "metadata": {
        "id": "XonPcqt9e5z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model\n",
        "model = QA_Transformer(vocab_size =len(tokenizer.vocab), d_model = 64, max_len = 64, heads = 4)\n",
        "# start_logits, end_logits = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
        "start_logits, end_logits = model(input['input_ids'])#, input['attention_mask'])"
      ],
      "metadata": {
        "id": "iVqu2XY370pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start_logits, end_logits"
      ],
      "metadata": {
        "id": "v205VN5Ok9tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get answer span\n",
        "start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "print(start_idx, end_idx)\n",
        "tokens = inputs['input_ids'].tolist()\n",
        "# print(tokens)\n",
        "answer = [tokenizer.reverse_vocab.get(t, '[UNK]') for t in tokens[start_idx:end_idx+1]]\n",
        "print(\"Predicted answer:\", \" \".join(answer))"
      ],
      "metadata": {
        "id": "dqyflOz6lC6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model has not been trained yet and no seed is set so output is very unstable\n",
        "\n",
        "### Output from the above:\n",
        "46 58\n",
        "Predicted answer: to the majority class among those neighbors. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
        "\n",
        "### But sometimes it could be empty."
      ],
      "metadata": {
        "id": "k5IvGBMglWoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Existing attention head from pytorch"
      ],
      "metadata": {
        "id": "oyHHQQOQjz-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[UNK]\": 3}\n",
        "        self.reverse_vocab = {0: \"[PAD]\", 1: \"[CLS]\", 2: \"[SEP]\", 3: \"[UNK]\"}\n",
        "        self.idx = 4\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        for text in texts:\n",
        "            for word in text.lower().split():\n",
        "                if word not in self.vocab:\n",
        "                    self.vocab[word] = self.idx\n",
        "                    self.reverse_vocab[self.idx] = word\n",
        "                    self.idx += 1\n",
        "\n",
        "    def encode(self, question, context, max_len=64):\n",
        "        tokens = [\"[CLS]\"] + question.lower().split() + [\"[SEP]\"] + context.lower().split() + [\"[SEP]\"]\n",
        "        token_ids = [self.vocab.get(token, self.vocab[\"[UNK]\"]) for token in tokens]\n",
        "        attention_mask = [1] * len(token_ids)\n",
        "        padding = [0] * (max_len - len(token_ids))\n",
        "        return (\n",
        "            torch.tensor(token_ids + padding[:max_len - len(token_ids)]),\n",
        "            torch.tensor(attention_mask + padding[:max_len - len(token_ids)])\n",
        "        )\n"
      ],
      "metadata": {
        "id": "FL2SroxwguIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # print(x)\n",
        "        attn_output, _ = self.attn(x, x, x, key_padding_mask=~mask.bool())\n",
        "        # print(attn_output)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VFCJLB62-df4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QA_Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, max_len=64, heads=4):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "        self.encoder = TransformerBlock(d_model, heads)\n",
        "        self.qa_outputs = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1)]\n",
        "        # print(x.shape)\n",
        "        x = self.encoder(x, attention_mask)\n",
        "        # print(x.shape)\n",
        "        logits = self.qa_outputs(x)  # [batch, seq_len, 2]\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n"
      ],
      "metadata": {
        "id": "lTtXqNwigl-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "question = \"What is KNN?\"\n",
        "context = '''KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It classifies new data points by finding the \"k\" most similar data points (neighbors) in the training data and assigning the new data point to the majority class among those neighbors.'''\n",
        "\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.build_vocab([question, context])\n",
        "input_ids, attention_mask = tokenizer.encode(question, context)\n",
        "\n",
        "# Model\n",
        "model = QA_Transformer(vocab_size=len(tokenizer.vocab))\n",
        "start_logits, end_logits = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
        "\n",
        "# Get answer span\n",
        "start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "tokens = input_ids.tolist()\n",
        "answer = [tokenizer.reverse_vocab.get(t, '[UNK]') for t in tokens[start_idx:end_idx+1]]\n",
        "print(\"Predicted answer:\", \" \".join(answer))\n"
      ],
      "metadata": {
        "id": "aQWToSpdgndv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask"
      ],
      "metadata": {
        "id": "P08qZCBsgo5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[start_idx:end_idx+1]"
      ],
      "metadata": {
        "id": "RGac9X73gzOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model has not been trained yet and no seed is set so output is very unstable\n",
        "\n",
        "### output from above:\n",
        "Predicted answer: is a supervised machine learning algorithm used for both classification and regression tasks. it classifies new data points\n",
        "\n",
        "### But another run will be different\n",
        "### Model needs to be trained and set seeds"
      ],
      "metadata": {
        "id": "Vh_5Dw3ilckK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lfvHyPD4hhLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}