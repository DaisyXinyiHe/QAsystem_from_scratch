{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple transformer question and answer model needs:\n",
        "* Tokinizer (Here i am using BERT's way to tokenize beginning and end of sentences)\n",
        "* Transformer encoder\n",
        "* QA head (predict start and end position)\n",
        "* Training and inference logic\n",
        "\n"
      ],
      "metadata": {
        "id": "F64zJ7O3cbSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use self-built attention head"
      ],
      "metadata": {
        "id": "bLQxgKC9j5SV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTfvlb4ObTv1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "        \"vocab_size\": 45,\n",
        "        \"hidden_size\": 64,\n",
        "        \"max_position_embeddings\": 64,\n",
        "        \"num_attention_heads\": 4,\n",
        "        'intermediate_size':10,\n",
        "        'hidden_dropout_prob':0.01,\n",
        "        \"num_hidden_layers\": 12,\n",
        "        'mask':None\n",
        "    }"
      ],
      "metadata": {
        "id": "MBvVdTLxGQYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokinizer\n",
        "\n",
        "class SimpleTokinizer:\n",
        "  def __init__(self):\n",
        "    self.vocab = {\"[PAD]\":0, \"[CLS]\":1, \"[SEP]\":2,\"[UNK]\":3}\n",
        "    self.reverse_vocab = {0:\"[PAD]\", 1:\"[CLS]\", 2:\"[SEP]\",3:\"[UNK]\"}\n",
        "    self.idx = 4\n",
        "\n",
        "  def build_vocab(self,texts):\n",
        "    for text in texts:\n",
        "      for word in text.lower().split():\n",
        "        if word not in self.vocab:\n",
        "          self.vocab[word] = self.idx\n",
        "          self.reverse_vocab[self.idx] = word\n",
        "          self.idx+=1\n",
        "  def encode(self, question, contaxt, max_len = 64):\n",
        "    ## for each QA, input takes format of [CLS] question tokens [SEP] context tokens [SEP]\n",
        "    tokens = [\"[CLS]\"]+question.lower().split()+[\"[SEP]\"]+contaxt.lower().split()+[\"[SEP]\"]\n",
        "    token_ids = [self.vocab.get(token, self.vocab[\"[UNK]\"]) for token in tokens]\n",
        "    token_type_ids = [1]*(len(question.split())+2)+[2]*(len(context.split())+1)\n",
        "    attention_mask = [1] * len(token_ids)\n",
        "    padding = [0]*(max_len - len(token_ids))\n",
        "    # print(token_type_ids)\n",
        "    return {\n",
        "        'input_ids':torch.tensor(token_ids + padding[:max_len - len(token_ids)]),\n",
        "        'attention_mask':torch.tensor(attention_mask+padding[:max_len - len(token_ids)]),\n",
        "        'token':tokens+['[PAD]']*len(padding),\n",
        "        'token_type_ids':torch.tensor(token_type_ids+padding[:max_len - len(token_ids)]),\n",
        "\n",
        "    }"
      ],
      "metadata": {
        "id": "AWoGbLg3cehF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample\n",
        "question = \"What is KNN?\"\n",
        "context = '''KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It classifies new data points by finding the \"k\" most similar data points (neighbors) in the training data and assigning the new data point to the majority class among those neighbors.'''\n"
      ],
      "metadata": {
        "id": "sM_MmMgWGd8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenizer\n",
        "tokenizer = SimpleTokinizer()\n",
        "tokenizer.build_vocab([question, context])\n",
        "# input_ids, attention_mask = tokenizer.encode(question, context)\n",
        "inputs = tokenizer.encode(question, context)"
      ],
      "metadata": {
        "id": "ZsVyzA_WGd8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs"
      ],
      "metadata": {
        "id": "lN_yR2g3TJMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids = input_ids.unsqueeze(0)\n",
        "# attention_mask = attention_mask.unsqueeze(0)"
      ],
      "metadata": {
        "id": "hbgu9-c2J8zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config['mask'] = attention_mask"
      ],
      "metadata": {
        "id": "YYsPnOP2RGZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_ids.shape, attention_mask.shape"
      ],
      "metadata": {
        "id": "SUcbVzSYKz-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Atttention head\n",
        "\n",
        "def scaled_dot_product_attention(q,k,v, mask = None):\n",
        "  # print(q.shape,k.shape,v.shape)\n",
        "  dim_k = k.size(-1) ## embedding size\n",
        "  # print(dim_k)\n",
        "  # print(k.transpose(1,2).shape)\n",
        "  scores = torch.bmm(q,k.transpose(1,2)) / math.sqrt(dim_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0, -float('inf'))\n",
        "  weights = F.softmax(scores, dim=1)\n",
        "  attention_outputs = torch.bmm(weights, v)\n",
        "  return attention_outputs\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, head_dim, mask=None):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim, head_dim)\n",
        "    self.k = nn.Linear(embed_dim, head_dim)\n",
        "    self.v = nn.Linear(embed_dim, head_dim)\n",
        "    self.mask = mask\n",
        "\n",
        "  def forward(self,hidden_state):\n",
        "    attention_outputs = scaled_dot_product_attention(self.q(hidden_state),self.k(hidden_state),self.v(hidden_state), mask = self.mask)\n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    embed_dim = config['hidden_size']\n",
        "    num_heads = config['num_attention_heads']\n",
        "    head_dim = embed_dim // num_heads\n",
        "    mask = config['mask']\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(embed_dim, head_dim, mask) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self,hidden_state):\n",
        "    # print(hidden_state.shape)\n",
        "    # for h in self.heads:\n",
        "    #   print(h(hidden_state)[0][0].shape)\n",
        "    # print(self.heads)\n",
        "    x = torch.cat([h(hidden_state) for h in self.heads], dim = -1)\n",
        "    x = self.output_linear(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "upj8B52_feAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AttentionHead(config['hidden_size'], config['num_attention_heads'], config['mask'])"
      ],
      "metadata": {
        "id": "JcksK0yqYRfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attn = MultiHeadAttention(config)\n",
        "token_emb = nn.Embedding(config['vocab_size'], config['hidden_size'])\n",
        "input_embeds = token_emb(inputs['input_ids'])\n",
        "input_embeds = input_embeds.unsqueeze(0)\n",
        "attn_output = multihead_attn(input_embeds)"
      ],
      "metadata": {
        "id": "dIL-noipHAsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config['hidden_size'], config['intermediate_size'])\n",
        "    self.linear2 = nn.Linear(config['intermediate_size'], config['hidden_size'])\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "jGqt0bAug0Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_forward = FeedForward(config)"
      ],
      "metadata": {
        "id": "DKwjjqd0PTjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff_outputs = feed_forward(attn_output)"
      ],
      "metadata": {
        "id": "_S64BQZvPVG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff_outputs.shape"
      ],
      "metadata": {
        "id": "rk4RkliVPWXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.layer_norm1 = nn.LayerNorm(config['hidden_size'])\n",
        "    self.layer_norm2 = nn.LayerNorm(config['hidden_size'])\n",
        "    self.attention = MultiHeadAttention(config)\n",
        "    self.feedforward = FeedForward(config)\n",
        "  def forward(self, x):\n",
        "    hidden_state = self.layer_norm1(x)\n",
        "    atten_output =  self.attention(hidden_state)\n",
        "    x+=atten_output\n",
        "    x += self.feedforward(self.layer_norm2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "6brH7YThmSMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = TransformerEncoderLayer(config)\n",
        "print(input_embeds.shape)\n",
        "encoder_layer(input_embeds).shape"
      ],
      "metadata": {
        "id": "E_9yHmzsPZ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.token_embeddings = nn.Embedding(config['vocab_size'], config['hidden_size'])\n",
        "    self.position_embeddings = nn.Embedding(config['max_position_embeddings'], config['hidden_size'])\n",
        "    self.layer_norm = nn.LayerNorm(config['hidden_size'], eps = 1e-12)\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    seq_length = input_ids.unsqueeze(0).size(1)\n",
        "    position_ids = torch.arange(seq_length, dtype = torch.long).unsqueeze(0)\n",
        "    # print(input_ids)\n",
        "    token_embeddings = self.token_embeddings(input_ids)\n",
        "    position_embeddings = self.position_embeddings(position_ids)\n",
        "    embeddings = token_embeddings+position_embeddings\n",
        "    embeddings = self.layer_norm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "VQ4V-vC92JPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embeddings(config)\n",
        "embedding_layer(inputs['input_ids'])#.size()"
      ],
      "metadata": {
        "id": "jriCgnyGbYBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.embeddings = Embeddings(config)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [TransformerEncoderLayer(config) for _ in range(config['num_hidden_layers'])]\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.embeddings(x)\n",
        "    for layer in self.layers:\n",
        "      # print(layer)\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oMlQ6Sq14Lyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoder(config)\n",
        "encoder(inputs['input_ids']).size()"
      ],
      "metadata": {
        "id": "fkKToZBH6dRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add a QA head\n",
        "class QA_Transformer(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model = 64, max_len = 64, heads = 4):\n",
        "    super().__init__()\n",
        "    self.config = {\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"hidden_size\": d_model,\n",
        "        \"max_position_embeddings\": max_len,\n",
        "        \"num_attention_heads\": heads,\n",
        "        'intermediate_size':10,\n",
        "        'hidden_dropout_prob':0.01,\n",
        "        \"num_hidden_layers\": 12,\n",
        "        'mask':None\n",
        "    }\n",
        "    # self.embedding = nn.Embedding(self.config['vocab_size'], self.config['hidden_size'])\n",
        "    self.encoder = TransformerEncoder(self.config)\n",
        "    # self.position_embeddings = nn.Parameter(torch.randn(1, self.config['max_position_embeddings'], self.config['hidden_size']))\n",
        "    # self.position_embeddings = nn.Parameter(torch.randint(1, self.config['max_position_embeddings'], self.config['hidden_size']))\n",
        "    self.qa_outputs = nn.Linear(self.config['hidden_size'], 2)\n",
        "\n",
        "  def forward(self, input_ids):#, attention_mask):\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "    # attention_mask = attention_mask.unsqueeze(0)\n",
        "    # x = self.embedding(input_ids)+self.position_embeddings[:,:input_ids.size(1)]#.long()\n",
        "    x = self.encoder(input_ids)#, attention_mask)\n",
        "    logits = self.qa_outputs(x)\n",
        "    start_logits, end_logits = logits.split(1,dim=-1)\n",
        "    return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MpKwDhBJ4nnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample\n",
        "question = \"What is KNN?\"\n",
        "context = '''KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It classifies new data points by finding the \"k\" most similar data points (neighbors) in the training data and assigning the new data point to the majority class among those neighbors.'''\n"
      ],
      "metadata": {
        "id": "Lo3ZcEw56mP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenizer\n",
        "tokenizer = SimpleTokinizer()\n",
        "tokenizer.build_vocab([question, context])\n",
        "# input_ids, attention_mask = tokenizer.encode(question, context)\n",
        "# input_ids = input_ids.unsqueeze(0)\n",
        "# attention_mask = attention_mask.unsqueeze(0)\n",
        "input = tokenizer.encode(question, context)"
      ],
      "metadata": {
        "id": "D3ItMBvZ7RY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input['input_ids']"
      ],
      "metadata": {
        "id": "CTa0zsmO7oOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input['attention_mask'].shape, input['input_ids'].shape, len(tokenizer.vocab)"
      ],
      "metadata": {
        "id": "D_uBD0-07qi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input['input_ids'].shape"
      ],
      "metadata": {
        "id": "XonPcqt9e5z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model\n",
        "model = QA_Transformer(vocab_size =len(tokenizer.vocab), d_model = 64, max_len = 64, heads = 4)\n",
        "# start_logits, end_logits = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
        "start_logits, end_logits = model(input['input_ids'])#, input['attention_mask'])"
      ],
      "metadata": {
        "id": "iVqu2XY370pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start_logits, end_logits"
      ],
      "metadata": {
        "id": "v205VN5Ok9tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get answer span\n",
        "start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "print(start_idx, end_idx)\n",
        "tokens = inputs['input_ids'].tolist()\n",
        "# print(tokens)\n",
        "answer = [tokenizer.reverse_vocab.get(t, '[UNK]') for t in tokens[start_idx:end_idx+1]]\n",
        "print(\"Predicted answer:\", \" \".join(answer))"
      ],
      "metadata": {
        "id": "dqyflOz6lC6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model has not been trained yet and no seed is set so output is very unstable\n",
        "\n",
        "### Output from the above:\n",
        "46 58\n",
        "Predicted answer: to the majority class among those neighbors. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
        "\n",
        "### But sometimes it could be empty."
      ],
      "metadata": {
        "id": "k5IvGBMglWoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Existing attention head from pytorch"
      ],
      "metadata": {
        "id": "oyHHQQOQjz-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {\"[PAD]\": 0, \"[CLS]\": 1, \"[SEP]\": 2, \"[UNK]\": 3}\n",
        "        self.reverse_vocab = {0: \"[PAD]\", 1: \"[CLS]\", 2: \"[SEP]\", 3: \"[UNK]\"}\n",
        "        self.idx = 4\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        for text in texts:\n",
        "            for word in text.lower().split():\n",
        "                if word not in self.vocab:\n",
        "                    self.vocab[word] = self.idx\n",
        "                    self.reverse_vocab[self.idx] = word\n",
        "                    self.idx += 1\n",
        "\n",
        "    def encode(self, question, context, max_len=64):\n",
        "        tokens = [\"[CLS]\"] + question.lower().split() + [\"[SEP]\"] + context.lower().split() + [\"[SEP]\"]\n",
        "        token_ids = [self.vocab.get(token, self.vocab[\"[UNK]\"]) for token in tokens]\n",
        "        attention_mask = [1] * len(token_ids)\n",
        "        padding = [0] * (max_len - len(token_ids))\n",
        "        return (\n",
        "            torch.tensor(token_ids + padding[:max_len - len(token_ids)]),\n",
        "            torch.tensor(attention_mask + padding[:max_len - len(token_ids)])\n",
        "        )\n"
      ],
      "metadata": {
        "id": "FL2SroxwguIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # print(x)\n",
        "        attn_output, _ = self.attn(x, x, x, key_padding_mask=~mask.bool())\n",
        "        # print(attn_output)\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + ff_output)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VFCJLB62-df4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QA_Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, max_len=64, heads=4):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "        self.encoder = TransformerBlock(d_model, heads)\n",
        "        self.qa_outputs = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1)]\n",
        "        # print(x.shape)\n",
        "        x = self.encoder(x, attention_mask)\n",
        "        # print(x.shape)\n",
        "        logits = self.qa_outputs(x)  # [batch, seq_len, 2]\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n"
      ],
      "metadata": {
        "id": "lTtXqNwigl-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "question = \"What is KNN?\"\n",
        "context = '''KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for both classification and regression tasks. It classifies new data points by finding the \"k\" most similar data points (neighbors) in the training data and assigning the new data point to the majority class among those neighbors.'''\n",
        "\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = SimpleTokenizer()\n",
        "tokenizer.build_vocab([question, context])\n",
        "input_ids, attention_mask = tokenizer.encode(question, context)\n",
        "\n",
        "# Model\n",
        "model = QA_Transformer(vocab_size=len(tokenizer.vocab))\n",
        "start_logits, end_logits = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
        "\n",
        "# Get answer span\n",
        "start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "tokens = input_ids.tolist()\n",
        "answer = [tokenizer.reverse_vocab.get(t, '[UNK]') for t in tokens[start_idx:end_idx+1]]\n",
        "print(\"Predicted answer:\", \" \".join(answer))\n"
      ],
      "metadata": {
        "id": "aQWToSpdgndv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask"
      ],
      "metadata": {
        "id": "P08qZCBsgo5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[start_idx:end_idx+1]"
      ],
      "metadata": {
        "id": "RGac9X73gzOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model has not been trained yet and no seed is set so output is very unstable\n",
        "\n",
        "### output from above:\n",
        "Predicted answer: is a supervised machine learning algorithm used for both classification and regression tasks. it classifies new data points\n",
        "\n",
        "### But another run will be different\n",
        "### Model needs to be trained and set seeds"
      ],
      "metadata": {
        "id": "Vh_5Dw3ilckK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lfvHyPD4hhLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model with SQuAD data"
      ],
      "metadata": {
        "id": "p2G6AEDSwZov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "23G6byOMwcEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "## Load SQuAD data\n",
        "squad = load_dataset('squad')"
      ],
      "metadata": {
        "id": "igidq2_zwdeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squad"
      ],
      "metadata": {
        "id": "6GmNewbmw_Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = squad['train']\n",
        "val_data =squad['validation']"
      ],
      "metadata": {
        "id": "b381jPT9wtT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "1H8kiG4lw8fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "id": "32p85c-3w9oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[200]"
      ],
      "metadata": {
        "id": "vTwR2d_owzFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data[100]['question']"
      ],
      "metadata": {
        "id": "zC33AYdUw1Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_to_token_span(context, answer_start, answer_text, tokenizer):\n",
        "  words = context.lower().split()\n",
        "  char_idx = 0\n",
        "  token_start = token_end = -1\n",
        "  for i, word in enumerate(words):\n",
        "    if context.lower().find(answer_text.lower(), char_idx) != -1:\n",
        "      char_idx = context.lower().find(answer_text.lower(),char_idx)\n",
        "      token_start = len(context[:char_idx].split())\n",
        "      token_end = token_start+len(answer_text.split()) - 1\n",
        "      break\n",
        "  return token_start, token_end"
      ],
      "metadata": {
        "id": "XDP-Qbo_xEvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=64):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.samples = []\n",
        "\n",
        "        for item in data:\n",
        "            q = item[\"question\"]\n",
        "            c = item[\"context\"]\n",
        "            a = item[\"answers\"][\"text\"][0]\n",
        "            a_start = item[\"answers\"][\"answer_start\"][0]\n",
        "            self.tokenizer.build_vocab([q, c])\n",
        "            input_ids, attn_mask = tokenizer.encode(q, c, max_len)\n",
        "            start, end = char_to_token_span(c, a_start, a, tokenizer)\n",
        "\n",
        "            # Adjust for [CLS] and question tokens\n",
        "            offset = 1 + len(q.split()) + 1\n",
        "            start += offset\n",
        "            end += offset\n",
        "            if end >= max_len: continue  # discard too long\n",
        "\n",
        "            self.samples.append((input_ids, attn_mask, start, end))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids, mask, start, end = self.samples[idx]\n",
        "        input_ids = F.pad(input_ids, (0, self.max_len - input_ids.shape[0]), value=0)\n",
        "        mask = F.pad(mask, (0, self.max_len - mask.shape[0]), value=0)\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": mask,\n",
        "            \"start_pos\": torch.tensor(start),\n",
        "            \"end_pos\": torch.tensor(end)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "LR8E39nx0VkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "6w2O6C6cKmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(model, dataset, epochs=25, batch_size=16, lr=5e-4):\n",
        "tokenizer = SimpleTokenizer()\n",
        "train_dataset = QADataset(train_data.select(range(2000)), tokenizer)  # Use a small subset\n",
        "model = QA_Transformer(vocab_size=len(tokenizer.vocab))\n",
        "\n",
        "initial_weights = model.embedding.weight.clone().detach() # Store initial weights\n",
        "\n",
        "epochs=42\n",
        "batch_size=16\n",
        "lr=5e-4\n",
        "dataset = train_dataset\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "\n",
        "loss_hist = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        mask = batch[\"attention_mask\"]\n",
        "        start_pos = batch[\"start_pos\"]\n",
        "        end_pos = batch[\"end_pos\"]\n",
        "\n",
        "        start_logits, end_logits = model(input_ids, mask)\n",
        "        loss = loss_fn(start_logits, start_pos) + loss_fn(end_logits, end_pos)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}\")\n",
        "    loss_hist.append(total_loss / len(dataloader))\n"
      ],
      "metadata": {
        "id": "B-E5dJf-4-cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_weights = model.embedding.weight.clone().detach()"
      ],
      "metadata": {
        "id": "2j_SbGqGP4eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_weights"
      ],
      "metadata": {
        "id": "dyjsG4pIQCi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_weights"
      ],
      "metadata": {
        "id": "E_fODqGDQCk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_hist)"
      ],
      "metadata": {
        "id": "iBtFTNOUQ-kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step-by-step\n",
        "# tokenizer = SimpleTokenizer()\n",
        "# train_dataset = QADataset(train_data.select(range(2000)), tokenizer)  # Use a small subset\n",
        "# model = QA_Transformer(vocab_size=len(tokenizer.vocab))\n",
        "\n",
        "# train(model, train_dataset)\n"
      ],
      "metadata": {
        "id": "4j3fNt2h4_2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updated_weights = model.embedding.weight.clone().detach()"
      ],
      "metadata": {
        "id": "2e4TxGIZI_JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial_weights"
      ],
      "metadata": {
        "id": "lRA5mf65JO9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# updated_weights"
      ],
      "metadata": {
        "id": "ylBUqnc-JYs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HVDLGl6AJYMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(model, tokenizer, input_ids, attention_mask):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    start_logits, end_logits = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
        "    start = torch.argmax(start_logits, dim=1).item()\n",
        "    end = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    max_len = input_ids.size(0)\n",
        "    if start>end:\n",
        "      return \"\"\n",
        "    tokens = input_ids[start:end+1].tolist()\n",
        "    words = [tokenizer.reverse_vocab.get(t,'[UNK]') for t in tokens]\n",
        "    return \" \".join(words)\n"
      ],
      "metadata": {
        "id": "qg3TFTR8LIpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def normalize(text):\n",
        "  def remove_punc(s):\n",
        "    return \"\".join(c for c in s if c not in string.punctuation)\n",
        "\n",
        "  def remove_articles(s):\n",
        "    return \" \".join([w for w in s.split() if w not in [\"a\",'an','the']])\n",
        "  return remove_articles(remove_punc(text.lower())).strip()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "k01l2IsjRpzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1(pred, truth):\n",
        "  pred_tokens = normalize(pred).split()\n",
        "  truth_tokens = normalize(truth).split()\n",
        "  common = set(pred_tokens) & set(truth_tokens)\n",
        "  if len(common) == 0: return 0\n",
        "  precision = len(common) / len(pred_tokens)\n",
        "  recall = len(common) / len(truth_tokens)\n",
        "  return 2*(precision*recall) / (precision+recall)"
      ],
      "metadata": {
        "id": "8qYjajn4Synv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_em(pred,truth):\n",
        "  return int(normalize(pred) == normalize(truth))"
      ],
      "metadata": {
        "id": "jt6vHiLGVnwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataset, tokenizer, num_samples=100):\n",
        "  em_scores = []\n",
        "  f1_scores = []\n",
        "  for i in range(num_samples):\n",
        "    sample = dataset.data[i]\n",
        "    question = sample['question']\n",
        "    context = sample['context']\n",
        "    gt_answer = sample['answers']['text'][0]\n",
        "    input_ids, attention_mask = tokenizer.encode(question, context, max_len = 64)\n",
        "    input_ids = input_ids[:64]\n",
        "    attention_mask = attention_mask[:64]\n",
        "    pred_answer = predict_answer(model, tokenizer, input_ids, attention_mask)\n",
        "\n",
        "    em_scores.append(compute_em(pred_answer, gt_answer))\n",
        "    f1_scores.append(compute_f1(pred_answer, gt_answer))\n",
        "\n",
        "  avg_em = sum(em_scores) / len(em_scores)\n",
        "  avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "  print(f\"evaluate on {str(num_samples)} \\n\")\n",
        "  print(f'exact match: {avg_em:.2%}')\n",
        "  print(f'f1 score: {avg_f1:.2%}')\n",
        "  return avg_em, avg_f1"
      ],
      "metadata": {
        "id": "XVdMXz4sW2Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Val dataset has tokens that are not included in training dataset, hence causing errors\n",
        "## Might need to train data with both val and train\n",
        "## Or lemmentization or or better tokenization methods\n",
        "# val_subset = val_data.select(range(100))\n",
        "# val_dataset = QADataset(val_subset, tokenizer)\n",
        "# evaluate(model, val_dataset, tokenizer)\n",
        "\n",
        "train_subset = train_data.select(range(100))\n",
        "train_dataset = QADataset(train_subset, tokenizer)\n",
        "evaluate(model, train_dataset, tokenizer)"
      ],
      "metadata": {
        "id": "pYTq36DsYNPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.data[1]"
      ],
      "metadata": {
        "id": "sATFsBAmYmO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask = tokenizer.encode(train_dataset.data[1]['question'], train_dataset.data[1]['context'], max_len = 64)\n",
        "input_ids = input_ids[:64]\n",
        "attention_mask = attention_mask[:64]\n",
        "pred_answer = predict_answer(model, tokenizer, input_ids, attention_mask)"
      ],
      "metadata": {
        "id": "GOEqUG10dCRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_answer"
      ],
      "metadata": {
        "id": "M6vx62zJdGCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uXLq9jedN0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}